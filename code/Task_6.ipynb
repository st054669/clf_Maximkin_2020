{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lines = list(open('./news_train.txt', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'C:/Users/Professional/Documents/GitHub/clf_Maximkin_2020/data/news_train.txt'\n",
    "TEST_PATH = 'C:/Users/Professional/Documents/GitHub/clf_Maximkin_2020/data/news_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sport\\tОвечкин пожертвовал детской хоккейной школе автомобиль\\tНападающий «Вашингтон Кэпиталз» Александр Овечкин передал детской хоккейной школе автомобиль, полученный им после окончания Матча всех звезд Национальной хоккейной лиги (НХЛ). Об этом сообщается на официальном сайте лиги.Автомобиль Honda Accord был подарен хоккеисту по решению спонсоров мероприятия. Игрок НХЛ пожертвовал машину спортивной школе Nova Cool Cats Special Hockey Inc., которая расположена в штате Вирджиния.Овечкин общается с 10-летней девочкой Анной Шоб с синдромом Дауна, которая занимается в этой школе и является поклонницей спортсмена. В сентябре форвард пообедал вместе с юной хоккеисткой в японском ресторане.Матч всех звезд НХЛ в Коламбусе (штат Огайо) завершился победой команды «Джонатана Тэйвза» над командой «Ника Фолиньо» со счетом 17:12. Овечкин выступал за проигравший коллектив. Россиянин отметился тремя результативными передачами.\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = list(open(TRAIN_PATH, 'r', encoding='utf-8'))\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'economics': 2080,\n",
       "         'business': 554,\n",
       "         'science': 2156,\n",
       "         'life': 2033,\n",
       "         'culture': 2053,\n",
       "         'sport': 2215,\n",
       "         'style': 284,\n",
       "         'media': 2111,\n",
       "         'forces': 1225,\n",
       "         'travel': 289})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([line.split('\\t')[0] for line in lines[:15000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://drive.google.com/file/d/1mG3tPS_59pANrgwd6T2IgnHWgph4vYbg/view?usp=sharing'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://drive.google.com/file/d/1mG3tPS_59pANrgwd6T2IgnHWgph4vYbg/view?usp=sharing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, regex_string = r'\\b\\w+\\b'):\n",
    "    return re.findall(regex_string, text.lower())\n",
    "\n",
    "def tokenize_dataset(lines, regex_string = r'\\b\\w+\\b'):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        label, headline, text = line.split('\\t')\n",
    "        headline = tokenize_text(headline, regex_string)\n",
    "        text = tokenize_text(text, regex_string)\n",
    "        result.append(dict())\n",
    "        result[-1]['label'] = label\n",
    "        result[-1]['headline'] = headline\n",
    "        result[-1]['text'] = text\n",
    "    return result\n",
    "\n",
    "def lemmatize_dataset(lines):\n",
    "    morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "    for line in lines:\n",
    "        line['headline'] = list(map(lambda x: morph_analyzer.parse(x)[0].normal_form, line['headline']))\n",
    "        line['text'] = list(map(lambda x: morph_analyzer.parse(x)[0].normal_form, line['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenize_dataset(lines)\n",
    "lemmatize_dataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'economics': 0,\n",
    "                'business': 1,\n",
    "                'science': 2,\n",
    "                'life': 3,\n",
    "                'culture': 4,\n",
    "                'sport': 5,\n",
    "                'style': 6,\n",
    "                'media': 7,\n",
    "                'forces': 8,\n",
    "                'travel': 9\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(labels, mapping_dict):\n",
    "    return labels.map(lambda label: mapping_dict[label]).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(train_df['text'].append(train_df['headline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('украина', 0.711722731590271), ('рф', 0.7109173536300659), ('страна', 0.6423488855361938), ('белоруссия', 0.6368809938430786), ('крым', 0.635071873664856), ('европа', 0.6152746677398682), ('греция', 0.6043877601623535), ('турция', 0.5781927108764648), ('российский', 0.5667814016342163), ('республика', 0.565057635307312)] \n",
      " [('великобритания', 0.7096855044364929), ('китай', 0.6480938792228699), ('франция', 0.6311972737312317), ('индия', 0.6072044372558594), ('япония', 0.5947538614273071), ('германия', 0.5647455453872681), ('австралия', 0.5645737051963806), ('американский', 0.5614012479782104), ('страна', 0.5591923594474792), ('израиль', 0.5215529203414917)] \n",
      " [('ios', 0.9275631904602051), ('android', 0.8822500705718994), ('firefox', 0.8690578937530518), ('iphone', 0.8636046648025513), ('phone', 0.8586814403533936), ('планшет', 0.845851719379425), ('ipad', 0.8457365036010742), ('galaxy', 0.8391407132148743), ('nexus', 0.8320471048355103), ('mini', 0.823755145072937)]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.wv.most_similar(positive=['россия']),'\\n',\n",
    "      word2vec.wv.most_similar(positive=['сша']), '\\n', \n",
    "      word2vec.wv.most_similar(positive=['windows']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_vectorize(X, word2vec):\n",
    "    word2vec_dict = dict(zip(word2vec.wv.index2word, word2vec.wv.vectors))\n",
    "    dim = len(next(iter(word2vec_dict.values())))        \n",
    "    \n",
    "    tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "    tfidf.fit(X)\n",
    "    max_idf = max(tfidf.idf_)\n",
    "    word2weight = defaultdict(\n",
    "            lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "    )\n",
    "    \n",
    "    return np.array(\n",
    "        [np.mean([word2vec[w] * word2weight[w] for w in words if w in word2vec] or [np.zeros(dim)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-fb9fd793412f>:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  [np.mean([word2vec[w] * word2weight[w] for w in words if w in word2vec] or [np.zeros(dim)], axis=0) for words in X])\n",
      "<ipython-input-51-fb9fd793412f>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  [np.mean([word2vec[w] * word2weight[w] for w in words if w in word2vec] or [np.zeros(dim)], axis=0) for words in X])\n"
     ]
    }
   ],
   "source": [
    "x_train = weighted_average_vectorize(train_df['text'], word2vec)\n",
    "y_train = map_labels(train_df['label'], mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lines = list(open(TEST_PATH, 'r', encoding='utf-8'))\n",
    "test_data = tokenize_dataset(test_lines)\n",
    "lemmatize_dataset(test_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-fb9fd793412f>:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  [np.mean([word2vec[w] * word2weight[w] for w in words if w in word2vec] or [np.zeros(dim)], axis=0) for words in X])\n",
      "<ipython-input-51-fb9fd793412f>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  [np.mean([word2vec[w] * word2weight[w] for w in words if w in word2vec] or [np.zeros(dim)], axis=0) for words in X])\n"
     ]
    }
   ],
   "source": [
    "x_test = weighted_average_vectorize(test_df['text'], word2vec)\n",
    "y_test = map_labels(test_df['label'], mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8433333333333334"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(n_jobs = -1)\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
